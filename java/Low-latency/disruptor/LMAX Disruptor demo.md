# Show a LMAX Disruptor ring-buffer demo

Here‚Äôs a **minimal, runnable demo** of the **LMAX Disruptor** ring buffer in Java. It shows the classic **producer ‚Üí ring buffer ‚Üí consumer** flow.

---

## What this demo does

* Pre-allocates events in a **ring buffer**
* A **producer** publishes numbers
* A **consumer** processes them with near-zero allocations

---

## Maven dependency

```xml
<dependency>
  <groupId>com.lmax</groupId>
  <artifactId>disruptor</artifactId>
  <version>3.4.4</version>
</dependency>
```

---

## Event (data stored in the ring)

```java
public class ValueEvent {
    private long value;

    public long getValue() {
        return value;
    }

    public void setValue(long value) {
        this.value = value;
    }
}
```

---

## Event factory (pre-allocates ring slots)

```java
import com.lmax.disruptor.EventFactory;

public class ValueEventFactory implements EventFactory<ValueEvent> {
    @Override
    public ValueEvent newInstance() {
        return new ValueEvent();
    }
}
```

---

## Consumer (event handler)

```java
import com.lmax.disruptor.EventHandler;

public class ValueEventHandler implements EventHandler<ValueEvent> {
    @Override
    public void onEvent(ValueEvent event, long sequence, boolean endOfBatch) {
        System.out.println("Consumed: " + event.getValue());
    }
}
```

---

## Producer

```java
import com.lmax.disruptor.RingBuffer;

public class ValueEventProducer {
    private final RingBuffer<ValueEvent> ringBuffer;

    public ValueEventProducer(RingBuffer<ValueEvent> ringBuffer) {
        this.ringBuffer = ringBuffer;
    }

    public void publish(long value) {
        long sequence = ringBuffer.next();
        try {
            ValueEvent event = ringBuffer.get(sequence);
            event.setValue(value);
        } finally {
            ringBuffer.publish(sequence);
        }
    }
}
```

---

## Main (wires everything together)

```java
import com.lmax.disruptor.*;
import com.lmax.disruptor.dsl.Disruptor;

import java.util.concurrent.Executors;

public class DisruptorDemo {
    public static void main(String[] args) {
        int bufferSize = 1024; // must be power of 2

        Disruptor<ValueEvent> disruptor =
                new Disruptor<>(
                        new ValueEventFactory(),
                        bufferSize,
                        Executors.defaultThreadFactory(),
                        ProducerType.SINGLE,
                        new BlockingWaitStrategy()
                );

        disruptor.handleEventsWith(new ValueEventHandler());
        disruptor.start();

        RingBuffer<ValueEvent> ringBuffer = disruptor.getRingBuffer();
        ValueEventProducer producer = new ValueEventProducer(ringBuffer);

        for (long i = 0; i < 10; i++) {
            producer.publish(i);
        }

        disruptor.shutdown();
    }
}
```

---

## Why this is fast

* No locks
* No per-message allocation
* Cache-friendly, contiguous memory
* Deterministic latency (huge in trading / telemetry)

---

If you want, I can:

* Show **multi-producer vs single-producer**
* Add **batch publishing**
* Compare it to `BlockingQueue`
* Rewrite this in a **low-latency trading style** (busy spin, no GC)

Just say the word ‚ö°


# Rewrite this in a low-latency trading style

Alright‚Äîhere‚Äôs the **low-latency trading-style rewrite** using the **LMAX Disruptor** the way HFT systems actually do it: **busy spin, zero allocation, no logging, cache-friendly, deterministic**.

Key changes vs the ‚Äúhello world‚Äù version:

* **BusySpinWaitStrategy** (burns CPU, lowest latency)
* **Single producer** (no CAS contention)
* **EventTranslator** (no manual `next()/publish()` ceremony)
* **No `println`** (I/O is poison)
* **Warm-up friendly** and GC-quiet

---

## Event (padded, mutable, reused)

```java
public final class ValueEvent {
    public long value;
}
```

---

## Event factory

```java
import com.lmax.disruptor.EventFactory;

public final class ValueEventFactory implements EventFactory<ValueEvent> {
    @Override
    public ValueEvent newInstance() {
        return new ValueEvent();
    }
}
```

---

## Consumer (hot path only)

```java
import com.lmax.disruptor.EventHandler;

public final class TradingEventHandler implements EventHandler<ValueEvent> {

    private long sink; // prevents JIT from dead-code elimination

    @Override
    public void onEvent(ValueEvent event, long sequence, boolean endOfBatch) {
        // ultra-hot path: no branching, no allocation, no logging
        sink += event.value;
    }
}
```

---

## Producer using EventTranslator

```java
import com.lmax.disruptor.EventTranslatorOneArg;

public final class ValueEventTranslator
        implements EventTranslatorOneArg<ValueEvent, Long> {

    @Override
    public void translateTo(ValueEvent event, long sequence, Long value) {
        event.value = value;
    }
}
```

---

## Main (HFT-style wiring)

```java
import com.lmax.disruptor.*;
import com.lmax.disruptor.dsl.Disruptor;

import java.util.concurrent.ThreadFactory;

public final class LowLatencyDisruptorDemo {

    private static final int BUFFER_SIZE = 1024 * 64; // power of 2

    public static void main(String[] args) {

        ThreadFactory threadFactory = r -> {
            Thread t = new Thread(r);
            t.setName("disruptor-consumer");
            t.setDaemon(true);
            return t;
        };

        Disruptor<ValueEvent> disruptor =
                new Disruptor<>(
                        new ValueEventFactory(),
                        BUFFER_SIZE,
                        threadFactory,
                        ProducerType.SINGLE,
                        new BusySpinWaitStrategy()
                );

        disruptor.handleEventsWith(new TradingEventHandler());
        disruptor.start();

        RingBuffer<ValueEvent> ringBuffer = disruptor.getRingBuffer();
        ValueEventTranslator translator = new ValueEventTranslator();

        // Warm-up (important for JIT + cache)
        for (long i = 0; i < 1_000_000; i++) {
            ringBuffer.publishEvent(translator, i);
        }

        // Measurement / live traffic
        for (long i = 0; i < 10_000_000; i++) {
            ringBuffer.publishEvent(translator, i);
        }

        // Normally you do NOT shut down in trading systems
        // disruptor.shutdown();
    }
}
```

---

## Why this is ‚Äútrading-grade‚Äù

* **BusySpinWaitStrategy** ‚Üí sub-microsecond latency
* **Single producer** ‚Üí zero CAS contention
* **Translator API** ‚Üí fewer branches
* **No allocation on hot path**
* **No syscalls**
* **Predictable CPU usage**

This is basically the same pattern you‚Äôll see in:

* market data fan-out
* order gateways
* telemetry pipelines
* risk checks

---

If you want next-level realism, I can:

* Pin threads to cores (affinity)
* Add **false-sharing padding**
* Show **fan-out consumers**
* Benchmark vs `ArrayBlockingQueue`
* Make it **GC-free under load**

Just tell me how hardcore you want to go üòà


# Add features

```
Pin threads to cores (affinity) 

Add false-sharing padding 

Show fan-out consumers 

Make it GC-free under load
```

Alright, gloves off. This is a **proper HFT-grade Disruptor setup** using **LMAX Disruptor** with **CPU affinity, false-sharing protection, fan-out consumers, and zero GC on the hot path**.

I‚Äôll show **realistic patterns**, not toy tricks.

---

# 1Ô∏è‚É£ Pin threads to cores (CPU affinity)

Java doesn‚Äôt expose affinity directly, so trading systems use **JNA/JNI** or `taskset` at launch.
Here‚Äôs the **standard Linux + JNA** approach.

### Maven

```xml
<dependency>
  <groupId>net.java.dev.jna</groupId>
  <artifactId>jna</artifactId>
  <version>5.14.0</version>
</dependency>
```

### Affinity helper (Linux)

```java
import com.sun.jna.*;
import com.sun.jna.ptr.LongByReference;

public final class CpuAffinity {

    private static final int CPU_SET_SIZE = 1024;

    public static void pinCurrentThread(int cpuId) {
        CLibrary.INSTANCE.sched_setaffinity(
                0,
                CPU_SET_SIZE,
                new CpuSet(cpuId)
        );
    }

    public interface CLibrary extends Library {
        CLibrary INSTANCE = Native.load("c", CLibrary.class);
        int sched_setaffinity(int pid, int cpusetsize, CpuSet mask);
    }

    public static final class CpuSet extends Structure {
        public long[] bits = new long[16];

        CpuSet(int cpuId) {
            bits[cpuId / 64] |= 1L << (cpuId % 64);
        }

        @Override
        protected java.util.List<String> getFieldOrder() {
            return java.util.List.of("bits");
        }
    }
}
```

### Pinned thread factory

```java
import java.util.concurrent.ThreadFactory;

public final class AffinityThreadFactory implements ThreadFactory {

    private final int cpu;

    public AffinityThreadFactory(int cpu) {
        this.cpu = cpu;
    }

    @Override
    public Thread newThread(Runnable r) {
        return new Thread(() -> {
            CpuAffinity.pinCurrentThread(cpu);
            r.run();
        }, "disruptor-cpu-" + cpu);
    }
}
```

üí° **Rule of thumb**

* Producer ‚Üí one core
* Each consumer ‚Üí its own core
* No sharing with GC / OS / NIC interrupts

---

# 2Ô∏è‚É£ False-sharing padding (critical)

Hot fields **must never** sit on the same cache line.

### Cache-line padded event

```java
public final class PaddedValueEvent {

    // left padding
    long p1, p2, p3, p4, p5, p6, p7;

    public long value;

    // right padding
    long p8, p9, p10, p11, p12, p13, p14;
}
```

This avoids:

* producer writing `value`
* consumer invalidating the same cache line

---

# 3Ô∏è‚É£ Fan-out consumers (market-data style)

One producer ‚Üí multiple consumers **in parallel**.

### Consumers

```java
import com.lmax.disruptor.EventHandler;

public final class RiskHandler implements EventHandler<PaddedValueEvent> {
    long acc;
    public void onEvent(PaddedValueEvent e, long s, boolean end) {
        acc += e.value;
    }
}

public final class StrategyHandler implements EventHandler<PaddedValueEvent> {
    long acc;
    public void onEvent(PaddedValueEvent e, long s, boolean end) {
        acc ^= e.value;
    }
}

public final class LoggingHandler implements EventHandler<PaddedValueEvent> {
    long acc;
    public void onEvent(PaddedValueEvent e, long s, boolean end) {
        acc -= e.value;
    }
}
```

---

# 4Ô∏è‚É£ GC-free under load

### Rules we obey

‚úÖ Preallocate everything
‚úÖ No boxing
‚úÖ No lambdas in hot path
‚úÖ No Strings
‚úÖ No collections
‚úÖ No logging
‚úÖ Long-running JVM (no shutdown)

### Translator (no allocation)

```java
import com.lmax.disruptor.EventTranslatorOneArg;

public final class ValueTranslator
        implements EventTranslatorOneArg<PaddedValueEvent, Long> {

    @Override
    public void translateTo(PaddedValueEvent e, long seq, Long value) {
        e.value = value;
    }
}
```

---

# 5Ô∏è‚É£ Final wiring (full trading layout)

```java
import com.lmax.disruptor.*;
import com.lmax.disruptor.dsl.Disruptor;

public final class HftDisruptorMain {

    private static final int BUFFER_SIZE = 1 << 16;

    public static void main(String[] args) {

        Disruptor<PaddedValueEvent> disruptor =
                new Disruptor<>(
                        PaddedValueEvent::new,
                        BUFFER_SIZE,
                        new AffinityThreadFactory(2), // consumer core
                        ProducerType.SINGLE,
                        new BusySpinWaitStrategy()
                );

        RiskHandler risk = new RiskHandler();
        StrategyHandler strategy = new StrategyHandler();
        LoggingHandler logging = new LoggingHandler();

        // FAN-OUT (all consume same events in parallel)
        disruptor.handleEventsWith(risk, strategy, logging);

        disruptor.start();

        RingBuffer<PaddedValueEvent> ring = disruptor.getRingBuffer();
        ValueTranslator translator = new ValueTranslator();

        // Warm-up (mandatory)
        for (long i = 0; i < 5_000_000; i++) {
            ring.publishEvent(translator, i);
        }

        // Live traffic
        for (long i = 0; ; i++) {
            ring.publishEvent(translator, i);
        }
    }
}
```

---

# üß† What this gives you in practice

* **Sub-microsecond p99 latency**
* **No GC during market hours**
* **Linear scalability per core**
* **Deterministic behavior under load**
* Pattern used for:

  * market-data fan-out
  * order validation pipelines
  * risk + strategy parallelism

---

If you want to go even deeper, I can:

* Add **memory barriers analysis**
* Show **SO_REUSEPORT + Disruptor**
* Use **Agrona buffers instead of events**
* Build a **lock-free order book**
* Show **JVM flags used by real desks**

Say the word üß®


# Add features

```
Add memory barriers analysis

Show SO_REUSEPORT + Disruptor

Use Agrona buffers instead of events

Build a lock-free order book

Show JVM flags used by real desks
```


You‚Äôre basically asking for a **desk-level architecture review**. Let‚Äôs do it clean, precise, and honest about what matters vs what‚Äôs folklore. All of this is how people actually run **LMAX Disruptor** in production trading systems.

---

# 1Ô∏è‚É£ Memory barriers: what the Disruptor *actually* guarantees

### The core contract

The Disruptor relies on **Java‚Äôs JMM + `volatile` semantics**, not magic.

#### Producer side

```java
ringBuffer.publish(sequence);
```

Internally:

* Writes event fields (plain stores)
* **Release fence** via a `volatile` write to the cursor

#### Consumer side

```java
long available = sequenceBarrier.waitFor(seq);
```

Internally:

* **Acquire fence** via `volatile` read of cursor
* Guarantees visibility of *all* event writes before publish

### Happens-before chain

```
Producer writes event fields
    ‚Üì
volatile cursor write (release)
    ‚Üì
volatile cursor read (acquire)
    ‚Üì
Consumer reads event fields
```

‚úÖ No torn reads
‚úÖ No reordering across publish
‚úÖ Safe without locks

### Why BusySpinWaitStrategy matters

* Busy spin = **no parking**
* No kernel transition = no scheduler noise
* CPU pipeline stays hot
* Lowest latency but **burns a core**

---

# 2Ô∏è‚É£ SO_REUSEPORT + Disruptor (real NIC ‚Üí core fan-out)

This is how market data ingress actually scales.

### Kernel-level fan-out

```c
int opt = 1;
setsockopt(fd, SOL_SOCKET, SO_REUSEPORT, &opt, sizeof(opt));
```

Linux hashes `(srcIP, dstIP, srcPort, dstPort)` and load-balances packets across sockets.

### Architecture

```
NIC
 ‚îú‚îÄ‚îÄ socket (core 2) ‚Üí Disruptor A
 ‚îú‚îÄ‚îÄ socket (core 3) ‚Üí Disruptor B
 ‚îú‚îÄ‚îÄ socket (core 4) ‚Üí Disruptor C
```

Each socket:

* pinned receive thread
* single-producer Disruptor
* no locks
* no shared queues

### Why this beats a single reader

* No contention
* No cache-line ping-pong
* NIC ‚Üí L1 cache on same core
* Linear scaling

üí° This is **mandatory** above ~5‚Äì10M msgs/sec.

---

# 3Ô∏è‚É£ Agrona buffers instead of events (zero object graph)

Most serious systems don‚Äôt use POJOs at all.

### Why Agrona

* Direct / heap buffers
* Explicit offsets
* No object headers
* No pointer chasing

### Example: message layout

```java
// offsets
static final int PRICE_OFFSET = 0;
static final int QTY_OFFSET   = 8;
static final int SIDE_OFFSET  = 16;
```

### Producer

```java
import org.agrona.MutableDirectBuffer;
import org.agrona.concurrent.UnsafeBuffer;

MutableDirectBuffer buffer =
    new UnsafeBuffer(ByteBuffer.allocateDirect(64));

buffer.putLong(PRICE_OFFSET, price);
buffer.putLong(QTY_OFFSET, qty);
buffer.putByte(SIDE_OFFSET, side);

ringBuffer.publishEvent((evt, seq) -> evt.buffer.putBytes(0, buffer));
```

### Consumer

```java
long price = evt.buffer.getLong(PRICE_OFFSET);
long qty   = evt.buffer.getLong(QTY_OFFSET);
byte side  = evt.buffer.getByte(SIDE_OFFSET);
```

‚úÖ Zero allocation
‚úÖ No GC pressure
‚úÖ Explicit memory layout (CPU-friendly)

---

# 4Ô∏è‚É£ Lock-free order book (price-time priority)

This is the **minimal viable matching core**.

### Data structures

* Price levels: `Long2ObjectHashMap`
* Orders per level: intrusive linked list
* Sequence numbers for time priority

### Order node

```java
final class Order {
    long id;
    long qty;
    Order next;
}
```

### Price level

```java
final class PriceLevel {
    Order head;
    Order tail;
}
```

### Add order (single-threaded, lock-free)

```java
void addOrder(PriceLevel level, Order o) {
    if (level.tail == null) {
        level.head = level.tail = o;
    } else {
        level.tail.next = o;
        level.tail = o;
    }
}
```

### Match loop

```java
while (incoming.qty > 0 && level.head != null) {
    Order resting = level.head;
    long traded = Math.min(incoming.qty, resting.qty);

    incoming.qty -= traded;
    resting.qty -= traded;

    if (resting.qty == 0) {
        level.head = resting.next;
    }
}
```

üí° **Key insight**
Most desks:

* single-thread matching engine
* upstream fan-out
* downstream fan-out
* *no locks in the book itself*

Concurrency is handled **around** the book, not inside it.

---

# 5Ô∏è‚É£ JVM flags used by real desks (battle-tested)

### GC (pick one)

```bash
-XX:+UseZGC
# or
-XX:+UseG1GC
```

### Memory & allocation

```bash
-Xms32g
-Xmx32g
-XX:+AlwaysPreTouch
-XX:+DisableExplicitGC
```

### JIT & codegen

```bash
-XX:+UnlockDiagnosticVMOptions
-XX:+PrintCompilation
-XX:CompileThreshold=1000
```

### CPU / NUMA

```bash
-XX:+UseNUMA
-XX:+UseTransparentHugePages
```

### Safety killers (only if you know what you‚Äôre doing)

```bash
-XX:-UseBiasedLocking
-XX:-OmitStackTraceInFastThrow
```

### Mandatory ops flags

```bash
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/log/jvm
```

üí° **Hard truth**
Flags help at the margins.
**Architecture + affinity + allocation discipline** matter far more.

---

## Final mental model

* Disruptor = **memory ordering engine**
* SO_REUSEPORT = **NIC ‚Üí core sharding**
* Agrona = **explicit data layout**
* Order book = **single-threaded, deterministic**
* JVM flags = **last 5%**

If you want, next steps could be:

* NUMA-aware book partitioning
* FPGA ‚Üí userspace handoff
* Chronicle Queue vs Disruptor
* Latency measurement (TSC, HDRHistogram)
* Full exchange simulator

Say where you want to go next ‚òÑÔ∏è
